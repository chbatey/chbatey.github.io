---
layout: post
title: 'Talk: Introduction to Cassandra, CQL, and the Datstax Java Driver'
date: '2013-09-25T00:09:00.000-07:00'
author: Christopher Batey
tags:
- talk
- datastax
- cassandra
- java
modified_time: '2013-09-25T11:44:52.860-07:00'
blogger_id: tag:blogger.com,1999:blog-4161315644722406995.post-4469426960254070602
blogger_orig_url: http://christopher-batey.blogspot.com/2013/09/talk-introduction-to-cassandra-cql-and.html
---

<div class="p1">
    <div class="p1"><b>Presenter</b>: Johnny Miller</div>
    <div class="p1"><b>Who is he?</b>&nbsp;Datastax Solutions Architect</div>
    <div class="p1"><b>Where?</b>&nbsp;Skills matter exchange in London</div>
    <div class="p1"><br/></div>
    <div class="p1">I went to an "introductory" talk even though I have a lot of experience with Cassandra for a few
        reasons:
    </div>
    <ul>
        <li>Meet other people in London that are using Cassandra</li>
        <li>To discover what I don't know about Cassandra</li>
    </ul>
    <div class="p1"></div>
    <div class="p1">Here are my notes that in roughly the same order as the talk.<br/><br/></div>
    <h3>What's Cassandra? The headlines</h3>
    <div class="p1"></div>
    <ul>
        <li>Been around for ~5 years - originally developed by Facebook for their inbox search</li>
        <li>Distributed key store - column orientated data model</li>
        <li>Tuneable consistency - per request decide how consistent you want the response to be</li>
        <li>Datacenter aware with asynchronous replication</li>
        <li>Designed for use as a cluster - not much value in a single node Cassandra deployment</li>
    </ul>
    <h3>Gossip - how nodes in a cluster learn about other nodes</h3>
    <ul class="ul1">
        <li class="li1">P2P protocol for how nodes discover location and state of other nodes</li>
        <li class="li1">New nodes are given seed nodes for bootstrapping - but these aren't single points of failure as
            they aren't used again
        </li>
    </ul>
    <h3>Data distribution and replication</h3>
    <ul class="ul1">
        <li class="li1">Replication factor: How many nodes each piece of data is stored on</li>
        <li class="li1">Each node is given a range of primary keys to look after</li>
    </ul>
    <h3>Partitioners - How to decide which node gets what data</h3>
    <ul class="ul1">
        <li class="li1">Row keys are hashed to decide node then a replication strategy defines how to pick the other
            replicas
        </li>
    </ul>
    <h3>Replicas - how to select where else the data lives</h3>
    <ul class="ul1">
        <li class="li1">All replicas are equally important. No difference between the node the key hashed to and the
            other replicas that were selected
        </li>
        <li class="li1">Two ways to pick the other replicas:</li>
        <ul class="ul1">
            <li class="li1">Simple: Only single DC. Specify just a replication factor. Hashes the key and then walks the
                cluster and picks the replicas. Not very clever - all replicas could end up on the same rack
            </li>
            <li class="li1">Network: Configure with a RF per DC. Walk the ring for each DC until it reaches a node in
                another rack
            </li>
        </ul>
    </ul>
    <h3>Snitch - how to define a data centre and a rack</h3>
    <ul class="ul1">
        <li class="li1">Informs Cassandra about node topology, designates DC and Rack for every node</li>
        <li class="li1">Example: Rack inferring snitch designates DC and Rack based on the IP of the node&nbsp;</li>
        <li class="li1">Example: Property file snitch where every node has the DC and Rack of every other node</li>
        <li class="li1">Example: GossipingPropertyFileSnitch: Every node knows its own DC and Rack and tells other nodes
            via Gossip
        </li>
        <li class="li1">Dynamic snitching: monitors performance of reads, this snitch wraps the other snitches to
            respond to network latency
        </li>
    </ul>
    <h3>Client requests</h3>
    <ul class="ul1">
        <li class="li1">Connect to any client in the node - becomes the coordinator. This node knows which nodes to talk
            to for the request
        </li>
        <li class="li1">Multi DC - picks a coordinator in the other data centre &nbsp;to replicate data there or to get
            data for a read
        </li>
    </ul>
    <h3>Consistency</h3>
    <ul class="ul1">
        <li class="li1">Quorum = (Replication Factor/2) + 1 i.e. more than half</li>
        <li class="li1">E.g R = 3, Q = 2, tolerate 1 replica going down to continue reading and writing at Quorum</li>
        <li class="li1">Per request consistency - can decide certain writes are more important and require higher
            consistency than others
        </li>
        <li class="li1">Example consistency levels: ANY, ONE, TWO, THREE, QUORUM, EACH_QUORUM, LOCAL_QUORUM</li>
        <li class="li1">SERIAL: New in cassandra 2.0</li>
    </ul>
    <h3>Write requests - what happens?</h3>
    <ul class="ul1">
        <li class="li1">The coordinator (node the client connects to) forwards the write to all the replicas in the
            local DC and designates a coordinator in the other DCs to do the same there
        </li>
        <li class="li1">The coordinator may be a replica but does not need to be</li>
        <li class="li1">For a single node writes first go to commit log (disk), then writes to meltable (memory)</li>
        <li class="li1">When does the write succeed? Depends on consistency e.g a write consistency of ONE means that
            the data needs to be in the commit log and memtable of at least one replica
        </li>
    </ul>
    <h3>Hinted handoff - how Cassandra deals with nodes being down on write</h3>
    <ul class="ul1">
        <li class="li1">Coordinator node keeps hints if one of the replicas down</li>
        <li class="li1">When the node comes back up the hints are then sent to the node so it can catch up</li>
        <li class="li1">Hints are kept for a finite amount of time - default is three hours</li>
    </ul>
    <h3>Read requests &nbsp;- what happens?</h3>
    <ul class="ul1">
        <li class="li1">Coordinator contacts a number of nodes depending on the consistency - once enough have responded
            the read can be successful&nbsp;</li>
        <li class="li1">Will send requests to node responding the fastest</li>
        <li class="li1">If not consistent - compare timestamps + do a read repair</li>
        <li class="li1">Possible other background read repair</li>
    </ul>
    <h3>What was missing?</h3>
    <div>Overall it was a great talk however here is some possible improvements:</div>
    <div class="p1"></div>
    <ul class="ul1">
        <li class="li1">A glossary/overview at the start? Perhaps a mapping from relational terminology to Cassandra
            terminology. For example the term keyspace was used a number of times before describing what it is
        </li>
        <li class="li1">Overview of consistency when talking about eventual consistency - however this did come later? A
            few scenarios for when read/writes at different consistency levels would fail/succeed would have been very
            helpful&nbsp;</li>
        <li class="li1">Compaction required for an intro talk? I thought talking about compaction was a bit too much for
            an introductory talk as you need to understand memtables and sstables before it makes sense
        </li>
        <li class="li1">The downsides of Cassandra: for example some forms of schema migration/change is a nightmare
            when you are using CQL3 + have data you need to migrate
        </li>
    </ul>
</div>
<ul class="ul1"></ul>