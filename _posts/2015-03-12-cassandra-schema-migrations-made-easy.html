---
layout: post
title: Cassandra schema migrations made easy with Apache Spark
date: '2015-03-12T23:30:00.000-07:00'
author: Christopher Batey
tags:
- schema
- spark
- cassandra
modified_time: '2015-03-12T23:30:12.565-07:00'
thumbnail: http://4.bp.blogspot.com/-ocpWDOp6ZHM/VOXt4fiOlII/AAAAAAAAAYQ/KBdlMbLjO4I/s72-c/oh-noes-everybody-panic.gif
blogger_id: tag:blogger.com,1999:blog-4161315644722406995.post-5929734782729177130
blogger_orig_url: http://christopher-batey.blogspot.com/2015/03/cassandra-schema-migrations-made-easy.html
---

By far the most common question I get asked when talking about Cassandra is once you've denormalised based on your queries what happens if you were wrong or a new requirement comes in that requires a new type of query.

<br/>
<br/>First I always check that it is a real requirement to be able to have this new functionality on old data. If that's not the case, and often it isn't, then you can just start double/triple writing into the new table.
<br/>
<br/>However if you truly need to have the new functionality on old data then Spark can come to the rescue. The first step is to still double write. We can then backfill using Spark. The awesome thing is that nearly all writes in Cassandra are idempotent, so when we backfill we don't need to worry about inserting data that was already inserted via the new write process.
<br/>
<br/>Let's see an example. Suppose you were storing customer events so you know what they are up to. At first you want to query by customer/time so you end up with following table:
<br/><br/>
<script src="https://gist.github.com/chbatey/15486be113cc2ed06f16.js"></script>
<br/>Then the requirement comes in to be able to look for events by staff member. My reaction a couple of years ago would have been something like this:
<br/><br/>
<div class="separator" style="clear: both; text-align: center;"><a
        href="http://4.bp.blogspot.com/-ocpWDOp6ZHM/VOXt4fiOlII/AAAAAAAAAYQ/KBdlMbLjO4I/s1600/oh-noes-everybody-panic.gif"
        imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0"
                                                                          src="http://4.bp.blogspot.com/-ocpWDOp6ZHM/VOXt4fiOlII/AAAAAAAAAYQ/KBdlMbLjO4I/s1600/oh-noes-everybody-panic.gif"/></a>
</div><br/>However if you have Spark workers on each of your Cassandra nodes then this is not an issue.<br/>
<br/>Assuming you want to a new table keyed by staff_id and have modified your application to double write you do the back fill with Spark. Here's the new table:
<br/><br/>
<script src="https://gist.github.com/chbatey/dff6f9e5a4363a2672ef.js"></script>
<br/>Then open up a Spark-shell (or submit a job) with the Spark-Cassandra connector on the classpath and all you'll need is something like this:
<br/><br/>
<script src="https://gist.github.com/chbatey/1286acb7dd2bf70873ec.js"></script>
<br/>How can a few lines do so much! If you're in a shell obviously you don't even need to create a SparkContext. What will happen here is the Spark workers will process the partitions on a Cassandra node that owns the data for the customer table (original table) and insert it back into Cassandra locally. Cassandra will then handle the replication to the correct nodes for the staff table.
<br/>
<br/>This is the least network traffic you could hope to achieve. Any solution that you write your self with Java/Python/Shell will involve pulling the data back to your application and pushing it to a new node, which will then need to replicate it for the new table.
<br/>
<br/>You won't want to do this at a peak time as this will HAMMER you Cassandra cluster as Spark is going to do this quickly. If you have a small DC for just running the Spark jobs and let it asynchronously replicate to your operational DC this is less of a concern.